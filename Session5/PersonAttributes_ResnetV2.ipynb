{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PersonAttrubutes.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/AI_Dataset/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, AveragePooling2D, Flatten\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs8a0Hrx6oLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "GDRIVE_MODEL_PATH = \"/content/gdrive/My Drive/AI_Dataset/resnetv2_model.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "# _cols_ = [col for col in one_hot_df.columns if not col.startswith(\"image_path\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augmentation = augmentation\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        \n",
        "        if self.augmentation is None:\n",
        "          image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        else:\n",
        "          image = np.stack([self.augmentation(image=cv2.imread(item[\"image_path\"]))[\"image\"] for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        # target = items[_cols_].values\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=30, shuffle=False)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from albumentations import (\n",
        "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
        "    RandomBrightness, RandomContrast, RandomGamma,\n",
        "    ToFloat, ShiftScaleRotate, Normalize, MotionBlur, Cutout, MedianBlur, CenterCrop, Blur\n",
        ")\n",
        "\n",
        "# create train and validation data generators\n",
        "training_augmentation = Compose([\n",
        "    HorizontalFlip(p=0.5),\n",
        "    RandomContrast(limit=0.5, p=0.5),\n",
        "    RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "    RandomBrightness(limit=0.5, p=0.5),\n",
        "    CLAHE(p=1.0),\n",
        "    #ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=0, p=0.8), \n",
        "    MotionBlur(blur_limit=7,p=0.6)\n",
        "    # ToFloat(max_value=255),\n",
        "    # Normalize()\n",
        "])\n",
        "\n",
        "#validation_augmentation = Compose([\n",
        "    # CLAHE(p=1.0, clip_limit=2.0),\n",
        "    #ToFloat(max_value=255)\n",
        "#])\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=22, augmentation=training_augmentation)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=22, shuffle=False, augmentation=None)\n",
        "\n",
        "cv2_imshow(train_gen[2][0][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8HCvlIhTGJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the total loss, category loss, and color loss\n",
        "lossNames = [\"gender_output_loss\", \"gender_output_acc\",\n",
        "             \"image_quality_output_loss\", \"image_quality_output_acc\",\n",
        "             \"age_output_loss\", \"age_output_acc\", \n",
        "             \"weight_output_loss\", \"weight_output_acc\", \n",
        "             \"bag_output_loss\", \"bag_output_acc\", \n",
        "             \"footwear_output_loss\", \"footwear_output_acc\", \n",
        "             \"pose_output_loss\", \"pose_output_acc\", \n",
        "             \"emotion_output_loss\", \"emotion_output_acc\"]\n",
        "def plot_model_losses(H, epochs):\n",
        "  plt.style.use(\"ggplot\")\n",
        "  (fig, ax) = plt.subplots(8, 2, figsize=(20, 45))\n",
        "  \n",
        "  # loop over the loss names\n",
        "  for (i, l) in enumerate(lossNames):\n",
        "    r = int(i/2)\n",
        "    c = i % 2\n",
        "    # plot the loss for both the training and validation data\n",
        "    title = format(l)\n",
        "    ax[r][c].set_title(title)\n",
        "    ax[r][c].set_xlabel(\"Epoch #\")\n",
        "    ax[r][c].set_ylabel(\"Loss\" if c == 1 else \"Accuracy\")\n",
        "    ax[r][c].plot(np.arange(0, epochs), H.history[l], label=l)\n",
        "    ax[r][c].plot(np.arange(0, epochs), H.history[\"val_\" + l],label=\"val_\" + l)\n",
        "    ax[r][c].set_xticks([])\n",
        "    ax[r][c].legend()\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B-cqFq2RrJwX"
      },
      "source": [
        "**ResNet V2 Definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMvD84pUcAU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.regularizers import l2\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    return x\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=10)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,kernel_initializer='he_normal')(y)\n",
        "    #outputs = y\n",
        "\n",
        "    # heads\n",
        "    gender = build_head(\"gender\", outputs)\n",
        "    image_quality = build_head(\"image_quality\", outputs)\n",
        "    age = build_head(\"age\", outputs)\n",
        "    weight = build_head(\"weight\", outputs)\n",
        "    bag = build_head(\"bag\", outputs)\n",
        "    footwear = build_head(\"footwear\", outputs)\n",
        "    emotion = build_head(\"emotion\", outputs)\n",
        "    pose = build_head(\"pose\", outputs)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion])\n",
        "    # model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2iWtKHIrbV4",
        "colab_type": "text"
      },
      "source": [
        "**Model Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CqPpfypSu2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade --upgrade-strategy only-if-needed https://github.com/faizanahemad/data-science-utils/tarball/master > /dev/null\n",
        "# !pip install --upgrade --upgrade-strategy only-if-needed git+https://www.github.com/keras-team/keras-contrib.git > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-_7yCH-rakg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        " return round(0.005 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "opt_sgd = SGD(lr=0.003, momentum=0.9)\n",
        "opt_sgd_nesterov = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "opt_adam = Adam(lr=0.003)\n",
        "\n",
        "losses = {\n",
        "  \"gender_output\": \"binary_crossentropy\",\n",
        "  \"image_quality_output\": \"categorical_crossentropy\",\n",
        "  \"age_output\": \"categorical_crossentropy\",\n",
        "  \"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\",\n",
        "\n",
        "}\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 20.0, \"pose_output\": 10.0, \"emotion_output\": 20.0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv0wPOLH4zCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = resnet_v2(input_shape=(224,224,3), depth=20, num_classes=27)\n",
        "model.compile(loss=losses,\n",
        "              loss_weights=loss_weights,\n",
        "              optimizer=opt_adam,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNOItKLBPWYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from data_science_utils.vision.keras import *\n",
        "\n",
        "# lrf = LRFinder(model)\n",
        "# model.metrics_names\n",
        "#generator = datagen.flow(X_train, Y_train, batch_size=256,shuffle=True)\n",
        "#test_generator = datagen_validation.flow(X_test, Y_test, batch_size=256, shuffle=True)\n",
        "# lrf.find_generator(train_gen, 0.0001, 10.0, valid_gen, epochs=1, steps_per_epoch=None)\n",
        "# lrf.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xDk8MVr8LQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTRXhNHjcsGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_gender_lr = ReduceLROnPlateau(monitor='gender_output_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "reduce_img_quality_lr = ReduceLROnPlateau(monitor='image_quality_output_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "reduce_age_lr = ReduceLROnPlateau(monitor='age_output_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "reduce_weight_lr = ReduceLROnPlateau(monitor='weight_output_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "reduce_bag_lr = ReduceLROnPlateau(monitor='bag_output_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "reduce_footwear_lr = ReduceLROnPlateau(monitor='footwear_output_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "reduce_pose_lr = ReduceLROnPlateau(monitor='pose_output_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "reduce_emotion_lr = ReduceLROnPlateau(monitor='emotion_output_loss', factor=0.2, patience=3, min_lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzYlA9PL4gLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    class_weight={0: 1., 1: 5., 2: 50., 3: 50., 4: 10., 5: 2., 6: 2., 7: 50.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXigVyvZTWjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.eval(model.optimizer.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j39Nyxr6d-BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=5,\n",
        "    verbose=1,\n",
        "    class_weight={0: 1., 1: 100., 2: 100., 3: 100., 4: 100., 5: 2., 6: 2., 7: 200.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqZ3ICz8E-PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    class_weight={0: 1., 1: 5., 2: 50., 3: 50., 4: 10., 5: 2., 6: 2., 7: 50.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlFGUFeWTsEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    class_weight={0: 1., 1: 5., 2: 50., 3: 50., 4: 10., 5: 2., 6: 2., 7: 200.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eob64g0svtlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(GDRIVE_MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lVhcnu8cGTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=5,\n",
        "    verbose=1,\n",
        "    class_weight={0: 20., 1: 100., 2: 100., 3: 100., 4: 100., 5: 1., 6: 1., 7: 50.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyWBb-mk8Sqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    class_weight={0: 20., 1: 100., 2: 100., 3: 100., 4: 100., 5: 1., 6: 1., 7: 50.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4bjfbG5EVON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.eval(model.optimizer.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dx8Rmb6HV-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(GDRIVE_MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yisXAhDMEtI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    class_weight={0: 1., 1: 100., 2: 100., 3: 50., 4: 50., 5: 1., 6: 1., 7: 100.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh4HqknCUih4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.eval(model.optimizer.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4kRfMhBU3YX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        " return round(0.002 * 1/(1 + 0.319 * epoch), 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cETdkhKCTgqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=20,\n",
        "    initial_epoch=10,\n",
        "    verbose=1,\n",
        "    class_weight={0: 1., 1: 100., 2: 100., 3: 50., 4: 50., 5: 1., 6: 1., 7: 100.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler, verbose=1)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR9LAN2HpSzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(GDRIVE_MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmND6BSQdh4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.eval(model.optimizer.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCnamdC5o6n0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=30,\n",
        "    initial_epoch=20,\n",
        "    verbose=1,\n",
        "    class_weight={0: 1., 1: 100., 2: 100., 3: 50., 4: 50., 5: 1., 6: 1., 7: 100.},\n",
        "    callbacks=[reduce_gender_lr,reduce_img_quality_lr,reduce_age_lr,reduce_weight_lr,\n",
        "               reduce_bag_lr,reduce_footwear_lr,reduce_pose_lr,reduce_emotion_lr,\n",
        "               LearningRateScheduler(scheduler, verbose=1)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Z69iNYsOAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(GDRIVE_MODEL_PATH)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8trcYOQ9YbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model):\n",
        "    results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "    accuracies = {}\n",
        "    losses = {}\n",
        "    for k, v in zip(model.metrics_names, results):\n",
        "        if k.endswith('acc'):\n",
        "            accuracies[k] = round(v * 100, 4) \n",
        "        else:\n",
        "            losses[k] = v\n",
        "    return accuracies\n",
        "\n",
        "evaluate_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}